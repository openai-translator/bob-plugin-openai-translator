// https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts

import { HttpResponse, TextTranslateQuery, ValidationCompletion } from "@bob-translate/types";

interface OpenAiChatCompletionDelta {
  role: 'assistant';
  content: string;
  /**
   * The refusal message generated by the model.
   */
  refusal: string | null;
}

/**
 * A chat completion message generated by the model.
 */
export interface OpenAiChatCompletionMessage {
  /**
   * The contents of the message.
   */
  content: string | null;

  /**
   * The refusal message generated by the model.
   */
  refusal: string | null;

  /**
   * The role of the author of this message.
   */
  role: 'assistant';
}


interface OpenAiChatCompletionChoice {
  index: number;
  delta: OpenAiChatCompletionDelta;
  /**
   * The refusal message generated by the model.
   */
  refusal: string | null;
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model
   * hit a natural stop point or a provided stop sequence, `length` if the maximum
   * number of tokens specified in the request was reached, `content_filter` if
   * content was omitted due to a flag from our content filters, `tool_calls` if the
   * model called a tool, or `function_call` (deprecated) if the model called a
   * function.
   */
  finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';

  /**
   * A chat completion message generated by the model.
   */
  message: OpenAiChatCompletionMessage;
}

export interface OpenAiChatCompletion {
  id: string;
  object: string;
  created: number;
  model: string;
  system_fingerprint?: string;
  choices: OpenAiChatCompletionChoice[];
}

/**
 * Describes an OpenAI model offering that can be used with the API.
 */
interface OpenAiModel {
  /**
   * The model identifier, which can be referenced in the API endpoints.
   */
  id: string;

  /**
   * The Unix timestamp (in seconds) when the model was created.
   */
  created: number;

  /**
   * The object type, which is always "model".
   */
  object: 'model';

  /**
   * The organization that owns the model.
   */
  owned_by: string;
}

export interface OpenAiModelList {
  object: string,
  data: OpenAiModel[]
}

export interface GeminiResponse {
  usageMetadata: {
    promptTokenCount: number;
    totalTokenCount: number;
    candidatesTokenCount: number;
  };
  modelVersion: string;
  candidates: Array<{
    content: {
      parts: Array<{
        text: string;
      }>;
      role: string;
    };
    finishReason: string;
    avgLogprobs: number;
  }>;
}

export interface ServiceAdapter {
  buildHeaders: (apiKey: string) => Record<string, string>;
  buildRequestBody: (query: TextTranslateQuery) => Record<string, unknown>;
  parseResponse: (response: HttpResponse<GeminiResponse | OpenAiChatCompletion>) => string;
  getTextGenerationUrl: (apiUrl: string) => string;
  testApiConnection: (
    apiKey: string,
    apiUrl: string,
    completion: ValidationCompletion,
  ) => Promise<void>;
  handleStream: (
    streamData: { text: string },
    query: TextTranslateQuery,
    targetText: string
  ) => string;
  makeStreamRequest: (
    url: string,
    header: Record<string, string>,
    body: Record<string, unknown>,
    query: TextTranslateQuery,
  ) => Promise<void>;
  makeRequest: (
    url: string,
    header: Record<string, string>,
    body: Record<string, unknown>,
    query: TextTranslateQuery,
  ) => Promise<void>;
  translate: (
    query: TextTranslateQuery,
    apiKey: string,
    apiUrl: string,
    isStream: boolean
  ) => Promise<void>;
}

export type ServiceProvider = 'azure-openai' | 'gemini' | 'openai' | 'openai-compatible';
